{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fb2b346-104f-4c59-9f24-ef955d00d3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "# Basic OS, date/time, and numerical libraries\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Libraries for data handling and scientific computing\n",
    "import xarray as xr\n",
    "import dask.array as da # For handling large arrays that don't fit in memory\n",
    "\n",
    "# PyTorch libraries for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F # For functions like F.pad\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# PyTorch Lightning for streamlining training\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "# Plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pandas for data manipulation (e.g., creating submission CSV)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70e1f3a3-b5e7-4d49-a588-1e1b836b99d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Core utilisation enabled (â€˜mediumâ€™).\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------\n",
    "# Cell 2 â€“ Configuration (ResNet)\n",
    "# --------------------------------\n",
    "config = {\n",
    "    \"data\": {\n",
    "        \"path\": \"processed_data_cse151b_v2_corrupted_ssp245/\"\n",
    "                \"processed_data_cse151b_v2_corrupted_ssp245.zarr\",\n",
    "        \"input_vars\":  [\"CO2\", \"SO2\", \"CH4\", \"BC\", \"rsdt\"],\n",
    "        \"output_vars\": [\"tas\", \"pr\"],\n",
    "        \"target_member_id\": 0,\n",
    "        \"train_ssps\": [\"ssp126\", \"ssp370\", \"ssp585\"],\n",
    "        \"test_ssp\":    \"ssp245\",\n",
    "        \"test_months\": 120,\n",
    "        \"batch_size\":  64,\n",
    "        \"num_workers\": 4,\n",
    "    },\n",
    "\n",
    "    # -------------  Model block -------------\n",
    "    \"model_resnet\": {                 # â† change key\n",
    "        \"type\": \"resnet\",             # â† will be parsed in your model-factory\n",
    "        \"arch\": \"resnet18\",           # resnet18 / 34 / 50 / 101 â€¦\n",
    "        \"in_channels\": 5,             # len(input_vars)\n",
    "        \"out_channels\": 2,            # len(output_vars)\n",
    "        \"pretrained\": False,          # True if using ImageNet weights\n",
    "        \"replace_first_conv\": True,   # set to True if in_channels â‰  3\n",
    "        \"fc_hidden\": 256,             # a small FC head before output layer\n",
    "        \"dropout\": 0.1,               # dropout prob in the head\n",
    "    },\n",
    "\n",
    "    # --------  Optimizer / training ---------\n",
    "    \"training\": {\n",
    "        \"optimizer\": \"AdamW\",         # AdamW works well with ResNets\n",
    "        \"lr\": 3e-4,                   # start a bit lower than 1e-3\n",
    "        \"weight_decay\": 1e-4,\n",
    "        \"lr_scheduler\": {\n",
    "            \"name\": \"CosineAnnealingLR\",\n",
    "            \"T_max\": 10,              # epochs for one cosine cycle\n",
    "            \"eta_min\": 1e-5,          # minimum LR\n",
    "        },\n",
    "        \"grad_clip\": 1.0,             # clip to stabilise large grads\n",
    "    },\n",
    "\n",
    "    # -------------  Trainer block -----------\n",
    "    \"trainer\": {\n",
    "        \"max_epochs\": 10,\n",
    "        \"accelerator\": \"auto\",\n",
    "        \"devices\":     \"auto\",\n",
    "        \"precision\":   16,            # try mixed precision for speed\n",
    "        \"deterministic\": True,\n",
    "        \"num_sanity_val_steps\": 0,\n",
    "        # Example callbacks (uncomment if you have them in code):\n",
    "        # \"callbacks\": [\n",
    "        #     {\"class_path\": \"pl.callbacks.ModelCheckpoint\",\n",
    "        #      \"init_args\": {\"save_top_k\": 2, \"monitor\": \"val_loss\"}},\n",
    "        #     {\"class_path\": \"pl.callbacks.EarlyStopping\",\n",
    "        #      \"init_args\": {\"monitor\": \"val_loss\", \"patience\": 5}}\n",
    "        # ],\n",
    "    },\n",
    "\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "# Set seeds, torch float-32 matmul precision hint\n",
    "pl.seed_everything(config[\"seed\"], workers=True)\n",
    "\n",
    "if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 7:\n",
    "    torch.set_float32_matmul_precision(\"medium\")\n",
    "    print(\"Tensor Core utilisation enabled (â€˜mediumâ€™).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "511ae3ce-bcaf-4d66-b482-fab75a25396c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Core utilisation enabled ('medium').\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------\n",
    "# Cell 2  â€“ Updated Configuration\n",
    "# --------------------------------\n",
    "config = {\n",
    "    # ---------- DATA ----------\n",
    "    \"data\": {\n",
    "        \"path\": \"processed_data_cse151b_v2_corrupted_ssp245/\"\n",
    "                \"processed_data_cse151b_v2_corrupted_ssp245.zarr\",\n",
    "        \"input_vars\":  [\"CO2\", \"SO2\", \"CH4\", \"BC\", \"rsdt\"],\n",
    "        \"output_vars\": [\"tas\", \"pr\"],\n",
    "        \"target_member_id\": 0,\n",
    "        \"train_ssps\": [\"ssp126\", \"ssp370\", \"ssp585\"],\n",
    "        \"test_ssp\":    \"ssp245\",\n",
    "        \"test_months\": 120,\n",
    "        \"batch_size\":  64,\n",
    "        \"num_workers\": 4,\n",
    "    },\n",
    "\n",
    "    # ---------- MODEL ----------\n",
    "    \"model_resnet\": {\n",
    "        \"type\":              \"resnet\",\n",
    "        \"arch\":              \"resnet18\",   # 18 / 34 / 50 / 101 / â€¦\n",
    "        \"in_channels\":        5,           # len(input_vars)\n",
    "        \"out_channels\":       2,           # len(output_vars)\n",
    "        \"pretrained\":         False,\n",
    "        \"replace_first_conv\": True,\n",
    "        \"fc_hidden\":         256,\n",
    "        \"dropout\":           0.10,\n",
    "    },\n",
    "\n",
    "    # ---------- OPTIMISER & SCHEDULER ----------\n",
    "    \"training\": {\n",
    "        \"optimizer\":      \"AdamW\",\n",
    "        \"optim_args\": {                  # â† anything here is passed to the optimiser\n",
    "            \"lr\":            3e-4,       # ðŸ¡ change learning-rate here\n",
    "            \"weight_decay\":  1e-4,       # ðŸ¡ change weight-decay here\n",
    "            \"betas\":         (0.9, 0.999)\n",
    "        },\n",
    "\n",
    "        # Cosine-annealing schedule (easy to swap)\n",
    "        \"lr_scheduler\": {\n",
    "            \"name\":   \"CosineAnnealingLR\",\n",
    "            \"T_max\":  10,                # one full cosine cycle over 10 epochs\n",
    "            \"eta_min\": 1e-5,\n",
    "        },\n",
    "\n",
    "        \"grad_clip\": 1.0,                # global-norm clip\n",
    "    },\n",
    "\n",
    "    # ---------- TRAINER ----------\n",
    "    \"trainer\": {\n",
    "        \"max_epochs\": 20,\n",
    "        \"accelerator\": \"auto\",\n",
    "        \"devices\":     \"auto\",\n",
    "        \"precision\":   16,               # AMP\n",
    "        \"deterministic\": True,\n",
    "        \"num_sanity_val_steps\": 0,\n",
    "        \"log_every_n_steps\": 10,\n",
    "\n",
    "        # Uncomment if you already import the callbacks elsewhere\n",
    "        # \"callbacks\": [\n",
    "        #     {\n",
    "        #         \"class_path\": \"lightning.pytorch.callbacks.ModelCheckpoint\",\n",
    "        #         \"init_args\":  {\"save_top_k\": 2, \"monitor\": \"val/loss\", \"mode\": \"min\"}\n",
    "        #     },\n",
    "        #     {\n",
    "        #         \"class_path\": \"lightning.pytorch.callbacks.EarlyStopping\",\n",
    "        #         \"init_args\":  {\"monitor\": \"val/loss\", \"patience\": 4, \"mode\": \"min\"}\n",
    "        #     }\n",
    "        # ],\n",
    "    },\n",
    "\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "# ------------- misc initialisation -------------\n",
    "pl.seed_everything(config[\"seed\"], workers=True)\n",
    "\n",
    "if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 7:\n",
    "    torch.set_float32_matmul_precision(\"medium\")\n",
    "    print(\"Tensor Core utilisation enabled ('medium').\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b8df60e-21e9-4e7b-a414-e1b5afde2aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Latitude Weights Utility\n",
    "\n",
    "def get_lat_weights(latitude_values):\n",
    "    \"\"\"\n",
    "    Computes cosine-based area weights for each latitude.\n",
    "    This accounts for the Earth's curvature, giving more weight to\n",
    "    grid cells near the equator for global metrics.\n",
    "\n",
    "    Args:\n",
    "        latitude_values (np.array): Array of latitude values in degrees.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Normalized latitude weights.\n",
    "    \"\"\"\n",
    "    lat_rad = np.deg2rad(latitude_values) # Convert degrees to radians\n",
    "    weights = np.cos(lat_rad)             # Cosine of latitude\n",
    "    return weights / np.mean(weights)     # Normalize by the mean weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "032abb3c-54cd-417d-93f8-9fe55d3a4892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Normalizer Class\n",
    "\n",
    "class Normalizer:\n",
    "    \"\"\"\n",
    "    Handles Z-score normalization for input and output data.\n",
    "    (data - mean) / std\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.mean_in, self.std_in = None, None   # Statistics for input data\n",
    "        self.mean_out, self.std_out = None, None # Statistics for output data\n",
    "\n",
    "    def set_input_statistics(self, mean, std):\n",
    "        \"\"\"Sets the mean and standard deviation for input data.\"\"\"\n",
    "        self.mean_in = mean\n",
    "        self.std_in = std\n",
    "\n",
    "    def set_output_statistics(self, mean, std):\n",
    "        \"\"\"Sets the mean and standard deviation for output data.\"\"\"\n",
    "        self.mean_out = mean\n",
    "        self.std_out = std\n",
    "\n",
    "    def normalize(self, data, data_type):\n",
    "        \"\"\"\n",
    "        Normalizes the data using pre-computed statistics.\n",
    "\n",
    "        Args:\n",
    "            data (np.array or dask.array): Data to normalize.\n",
    "            data_type (str): \"input\" or \"output\", to use appropriate statistics.\n",
    "\n",
    "        Returns:\n",
    "            Normalized data.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If statistics for the specified data_type are not set.\n",
    "        \"\"\"\n",
    "        if data_type == \"input\":\n",
    "            if self.mean_in is None or self.std_in is None:\n",
    "                raise ValueError(\"Input statistics not set in Normalizer.\")\n",
    "            # Add a small epsilon to std to prevent division by zero if std is very small or zero\n",
    "            return (data - self.mean_in) / (self.std_in + 1e-8) \n",
    "        elif data_type == \"output\":\n",
    "            if self.mean_out is None or self.std_out is None:\n",
    "                raise ValueError(\"Output statistics not set in Normalizer.\")\n",
    "            return (data - self.mean_out) / (self.std_out + 1e-8)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid data_type '{data_type}'. Must be 'input' or 'output'.\")\n",
    "\n",
    "    def inverse_transform_output(self, data):\n",
    "        \"\"\"\n",
    "        Applies inverse normalization to output data (predictions).\n",
    "\n",
    "        Args:\n",
    "            data (torch.Tensor or np.array): Normalized output data.\n",
    "\n",
    "        Returns:\n",
    "            Data in original physical units.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If output statistics are not set.\n",
    "        \"\"\"\n",
    "        if self.mean_out is None or self.std_out is None:\n",
    "            raise ValueError(\"Output statistics not set in Normalizer for inverse transform.\")\n",
    "        return data * (self.std_out + 1e-8) + self.mean_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e8e1fce-c1b0-43cb-9204-de384f73f2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Dense-output ResNet-FCN ----------------------------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# â”€â”€â”€ building blocks â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "    def __init__(self, in_c: int, out_c: int, stride: int = 1,\n",
    "                 downsample: nn.Module | None = None):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, 3, stride, 1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(out_c)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, 3, 1, 1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_c)\n",
    "        self.down  = downsample\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x if self.down is None else self.down(x)\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        return self.relu(out + identity)\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion: int = 4\n",
    "    def __init__(self, in_c: int, out_c: int, stride: int = 1,\n",
    "                 downsample: nn.Module | None = None):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, 1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(out_c)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, 3, stride, 1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_c)\n",
    "        self.conv3 = nn.Conv2d(out_c, out_c * self.expansion, 1, bias=False)\n",
    "        self.bn3   = nn.BatchNorm2d(out_c * self.expansion)\n",
    "        self.down  = downsample\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x if self.down is None else self.down(x)\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        return self.relu(out + identity)\n",
    "\n",
    "# â”€â”€â”€ ResNet-FCN backbone â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class ResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Returns dense (B, C_out, H, W) maps.\n",
    "    Works for input H,â€†W divisible by 8 (48Ã—72, 96Ã—144, â€¦).\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 depth: int              = 18,\n",
    "                 n_input_channels: int   = 5,\n",
    "                 n_output_classes: int   = 2):\n",
    "        super().__init__()\n",
    "\n",
    "        cfg = {18:  (BasicBlock,  [2, 2, 2, 2]),\n",
    "               34:  (BasicBlock,  [3, 4, 6, 3]),\n",
    "               50:  (Bottleneck,  [3, 4, 6, 3]),\n",
    "               101: (Bottleneck,  [3, 4, 23, 3]),\n",
    "               152: (Bottleneck,  [3, 8, 36, 3])}[depth]\n",
    "\n",
    "        block, layers = cfg\n",
    "        self.in_c = 64\n",
    "\n",
    "        # â”€â”€ stem (/2) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 64, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3, 2, 1)   # /2\n",
    "        )\n",
    "\n",
    "        # â”€â”€ residual stages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        self.layer1 = self._make_layer(block,  64, layers[0], stride=1)  # /2\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)  # /4\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)  # /8\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=1)  # keep /8\n",
    "\n",
    "        # â”€â”€ prediction head â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        self.head = nn.Conv2d(512 * block.expansion,\n",
    "                              n_output_classes, kernel_size=1)\n",
    "\n",
    "        # init\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "\n",
    "    def _make_layer(self, block, out_c, blocks, stride):\n",
    "        down = None\n",
    "        if stride != 1 or self.in_c != out_c * block.expansion:\n",
    "            down = nn.Sequential(\n",
    "                nn.Conv2d(self.in_c, out_c * block.expansion,\n",
    "                          1, stride, bias=False),\n",
    "                nn.BatchNorm2d(out_c * block.expansion)\n",
    "            )\n",
    "        layers = [block(self.in_c, out_c, stride, down)]\n",
    "        self.in_c = out_c * block.expansion\n",
    "        layers += [block(self.in_c, out_c) for _ in range(1, blocks)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    # â”€â”€ forward â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    def forward(self, x):\n",
    "        H, W = x.shape[-2:]\n",
    "        x = self.stem(x)\n",
    "        x = self.layer1(x); x = self.layer2(x)\n",
    "        x = self.layer3(x); x = self.layer4(x)\n",
    "        x = self.head(x)                       # (B, C_out, H/8, W/8)\n",
    "        return F.interpolate(x, (H, W),\n",
    "                             mode='bilinear',\n",
    "                             align_corners=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b7d3515-bfac-483c-af65-198f6dc952ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: ClimateDataset and ClimateDataModule\n",
    "\n",
    "class ClimateDataset(Dataset):\n",
    "    def __init__(self, inputs_dask, outputs_dask, output_is_normalized=True):\n",
    "        \"\"\"\n",
    "        PyTorch Dataset for climate data.\n",
    "\n",
    "        Args:\n",
    "            inputs_dask (dask.array): Dask array of input features.\n",
    "            outputs_dask (dask.array): Dask array of output targets.\n",
    "            output_is_normalized (bool): Flag indicating if outputs_dask is already normalized.\n",
    "                                         Used for the test set where targets are not pre-normalized.\n",
    "        \"\"\"\n",
    "        self.size = inputs_dask.shape[0]\n",
    "        print(f\"Creating dataset with {self.size} samples...\")\n",
    "\n",
    "        inputs_np = inputs_dask.compute()\n",
    "        outputs_np = outputs_dask.compute()\n",
    "\n",
    "        self.inputs = torch.from_numpy(inputs_np).float()\n",
    "        self.outputs = torch.from_numpy(outputs_np).float()\n",
    "\n",
    "        if torch.isnan(self.inputs).any():\n",
    "            raise ValueError(\"NaNs found in input dataset after converting to tensor.\")\n",
    "        if torch.isnan(self.outputs).any():\n",
    "            raise ValueError(\"NaNs found in output dataset after converting to tensor.\")\n",
    "        \n",
    "        print(f\"Dataset created. Input shape: {self.inputs.shape}, Output shape: {self.outputs.shape}\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.outputs[idx]\n",
    "\n",
    "\n",
    "class ClimateDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        path,\n",
    "        input_vars,\n",
    "        output_vars,\n",
    "        train_ssps,\n",
    "        test_ssp,\n",
    "        target_member_id,\n",
    "        test_months=120,\n",
    "        batch_size=32,\n",
    "        num_workers=0,\n",
    "        seed=42, \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() \n",
    "        self.normalizer = Normalizer()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        if not os.path.exists(self.hparams.path):\n",
    "            raise FileNotFoundError(f\"Data path not found: {self.hparams.path}. Please check config['data']['path'].\")\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        ds = xr.open_zarr(self.hparams.path, consolidated=False, chunks={\"time\": 24})\n",
    "        \n",
    "        # --- FIX for spatial_template ---\n",
    "        # The 'rsdt' variable might not have 'member_id'. Handle this conditionally.\n",
    "        rsdt_var_for_template = ds[\"rsdt\"]\n",
    "        if \"member_id\" in rsdt_var_for_template.dims:\n",
    "            spatial_template = rsdt_var_for_template.isel(time=0, ssp=0, member_id=0, drop=True)\n",
    "        else:\n",
    "            # If 'member_id' is not present, select without it.\n",
    "            # This assumes 'rsdt' is consistent across members or doesn't have that dimension.\n",
    "            spatial_template = rsdt_var_for_template.isel(time=0, ssp=0, drop=True)\n",
    "        # --- END FIX ---\n",
    "\n",
    "        def load_ssp(ssp_name):\n",
    "            input_dask_list, output_dask_list = [], []\n",
    "            \n",
    "            for var_name in self.hparams.input_vars:\n",
    "                da_var = ds[var_name].sel(ssp=ssp_name)\n",
    "                if \"latitude\" in da_var.dims: \n",
    "                    da_var = da_var.rename({\"latitude\": \"y\", \"longitude\": \"x\"})\n",
    "                # For input variables, if member_id exists, select the target_member_id.\n",
    "                # If it doesn't exist (e.g. for some forcing data), this sel will be a no-op if strict=False,\n",
    "                # or we can check existence. Xarray's sel is usually robust if the dim doesn't exist.\n",
    "                # However, to be safe, let's check.\n",
    "                if \"member_id\" in da_var.dims:\n",
    "                    da_var = da_var.sel(member_id=self.hparams.target_member_id)\n",
    "                \n",
    "                if set(da_var.dims) == {\"time\"}: \n",
    "                    da_var = da_var.broadcast_like(spatial_template).transpose(\"time\", \"y\", \"x\")\n",
    "                input_dask_list.append(da_var.data)\n",
    "            \n",
    "            for var_name in self.hparams.output_vars:\n",
    "                # Output variables are always selected by target_member_id\n",
    "                da_out = ds[var_name].sel(ssp=ssp_name, member_id=self.hparams.target_member_id)\n",
    "                if \"latitude\" in da_out.dims: \n",
    "                    da_out = da_out.rename({\"latitude\": \"y\", \"longitude\": \"x\"})\n",
    "                output_dask_list.append(da_out.data)\n",
    "\n",
    "            return da.stack(input_dask_list, axis=1), da.stack(output_dask_list, axis=1)\n",
    "\n",
    "        train_input_list, train_output_list = [], []\n",
    "        val_input_ssp370, val_output_ssp370 = None, None\n",
    "\n",
    "        for ssp in self.hparams.train_ssps:\n",
    "            x_ssp, y_ssp = load_ssp(ssp)\n",
    "            if ssp == \"ssp370\": \n",
    "                val_input_ssp370 = x_ssp[-self.hparams.test_months:]\n",
    "                val_output_ssp370 = y_ssp[-self.hparams.test_months:]\n",
    "                train_input_list.append(x_ssp[:-self.hparams.test_months])\n",
    "                train_output_list.append(y_ssp[:-self.hparams.test_months])\n",
    "            else:\n",
    "                train_input_list.append(x_ssp)\n",
    "                train_output_list.append(y_ssp)\n",
    "        \n",
    "        train_input_all_ssp = da.concatenate(train_input_list, axis=0)\n",
    "        train_output_all_ssp = da.concatenate(train_output_list, axis=0)\n",
    "\n",
    "        input_mean = da.nanmean(train_input_all_ssp, axis=(0, 2, 3), keepdims=True).compute()\n",
    "        input_std = da.nanstd(train_input_all_ssp, axis=(0, 2, 3), keepdims=True).compute()\n",
    "        self.normalizer.set_input_statistics(mean=input_mean, std=input_std)\n",
    "\n",
    "        output_mean = da.nanmean(train_output_all_ssp, axis=(0, 2, 3), keepdims=True).compute()\n",
    "        output_std = da.nanstd(train_output_all_ssp, axis=(0, 2, 3), keepdims=True).compute()\n",
    "        self.normalizer.set_output_statistics(mean=output_mean, std=output_std)\n",
    "\n",
    "        train_input_norm = self.normalizer.normalize(train_input_all_ssp, \"input\")\n",
    "        train_output_norm = self.normalizer.normalize(train_output_all_ssp, \"output\")\n",
    "        \n",
    "        val_input_norm = self.normalizer.normalize(val_input_ssp370, \"input\")\n",
    "        val_output_norm = self.normalizer.normalize(val_output_ssp370, \"output\")\n",
    "\n",
    "        test_input_ssp, test_output_ssp = load_ssp(self.hparams.test_ssp)\n",
    "        test_input_ssp = test_input_ssp[-self.hparams.test_months:] \n",
    "        test_output_ssp = test_output_ssp[-self.hparams.test_months:]\n",
    "        test_input_norm = self.normalizer.normalize(test_input_ssp, \"input\")\n",
    "\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.train_dataset = ClimateDataset(train_input_norm, train_output_norm)\n",
    "            self.val_dataset = ClimateDataset(val_input_norm, val_output_norm)\n",
    "        if stage == \"test\" or stage is None:\n",
    "            self.test_dataset = ClimateDataset(test_input_norm, test_output_ssp, output_is_normalized=False)\n",
    "        \n",
    "        self.lat = spatial_template.y.values\n",
    "        self.lon = spatial_template.x.values\n",
    "        self.area_weights = xr.DataArray(get_lat_weights(self.lat), dims=[\"y\"], coords={\"y\": self.lat})\n",
    "\n",
    "        ds.close()\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.hparams.batch_size, shuffle=True,\n",
    "                          num_workers=self.hparams.num_workers, pin_memory=torch.cuda.is_available(), persistent_workers=self.hparams.num_workers > 0)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.hparams.batch_size, shuffle=False,\n",
    "                          num_workers=self.hparams.num_workers, pin_memory=torch.cuda.is_available(), persistent_workers=self.hparams.num_workers > 0)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.hparams.batch_size, shuffle=False,\n",
    "                          num_workers=self.hparams.num_workers, pin_memory=torch.cuda.is_available(), persistent_workers=self.hparams.num_workers > 0)\n",
    "\n",
    "    def get_lat_weights(self):\n",
    "        return self.area_weights\n",
    "\n",
    "    def get_coords(self):\n",
    "        return self.lat, self.lon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e752dae9-03d3-4295-9c6f-408b1e2e4c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: ClimateEmulationModule (PyTorch Lightning)\n",
    "\n",
    "class ClimateEmulationModule(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate=1e-4):\n",
    "        super().__init__()\n",
    "        self.model = model \n",
    "        self.save_hyperparameters(ignore=['model']) \n",
    "        \n",
    "        self.criterion = nn.MSELoss() \n",
    "        self.normalizer = None \n",
    "        \n",
    "        self.val_preds, self.val_targets = [], []\n",
    "        self.test_preds, self.test_targets = [], []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def _get_normalizer_from_datamodule(self):\n",
    "        \"\"\"Helper to safely get normalizer from datamodule.\"\"\"\n",
    "        if hasattr(self.trainer, 'datamodule') and self.trainer.datamodule is not None and hasattr(self.trainer.datamodule, 'normalizer'):\n",
    "            return self.trainer.datamodule.normalizer\n",
    "        else:\n",
    "            # Fallback if trainer.datamodule is not set up (e.g. direct call to test without fit)\n",
    "            # This requires 'config' to be globally accessible or passed differently.\n",
    "            print(\"Warning: Normalizer not found via self.trainer.datamodule. Attempting fallback initialization.\")\n",
    "            temp_dm = ClimateDataModule(**config[\"data\"]) \n",
    "            temp_dm.prepare_data()\n",
    "            temp_dm.setup(stage=\"test\") # Or appropriate stage to ensure normalizer stats are computed\n",
    "            return temp_dm.normalizer\n",
    "\n",
    "\n",
    "    def on_fit_start(self):\n",
    "        \"\"\"Called at the beginning of training.\"\"\"\n",
    "        self.normalizer = self._get_normalizer_from_datamodule()\n",
    "\n",
    "    def on_test_start(self):\n",
    "        \"\"\"Called at the beginning of testing.\"\"\"\n",
    "        if self.normalizer is None: # Ensure normalizer is available\n",
    "            self.normalizer = self._get_normalizer_from_datamodule()\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y_norm = batch \n",
    "        y_hat_norm = self(x)   \n",
    "        loss = self.criterion(y_hat_norm, y_norm)\n",
    "        self.log(\"train/loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y_norm = batch\n",
    "        y_hat_norm = self(x)\n",
    "        loss = self.criterion(y_hat_norm, y_norm)\n",
    "        self.log(\"val/loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        if self.normalizer is None: self.normalizer = self._get_normalizer_from_datamodule()\n",
    "\n",
    "        y_hat_denorm = self.normalizer.inverse_transform_output(y_hat_norm.detach().cpu().numpy())\n",
    "        y_denorm = self.normalizer.inverse_transform_output(y_norm.detach().cpu().numpy())\n",
    "        \n",
    "        self.val_preds.append(y_hat_denorm)\n",
    "        self.val_targets.append(y_denorm)\n",
    "        return loss \n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if not self.trainer.sanity_checking: # Skip during sanity check\n",
    "            if not self.val_preds or not self.val_targets: \n",
    "                return\n",
    "\n",
    "            preds_epoch = np.concatenate(self.val_preds, axis=0)\n",
    "            trues_epoch = np.concatenate(self.val_targets, axis=0)\n",
    "            \n",
    "            if self.normalizer is None: self.normalizer = self._get_normalizer_from_datamodule()\n",
    "            \n",
    "            self._evaluate(preds_epoch, trues_epoch, phase=\"val\")\n",
    "            \n",
    "            np.save(\"val_preds.npy\", preds_epoch)\n",
    "            np.save(\"val_trues.npy\", trues_epoch)\n",
    "            \n",
    "            self.val_preds.clear() \n",
    "            self.val_targets.clear()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y_true_denorm = batch \n",
    "        y_hat_norm = self(x)    \n",
    "\n",
    "        if self.normalizer is None: self.normalizer = self._get_normalizer_from_datamodule()\n",
    "        \n",
    "        y_hat_denorm = self.normalizer.inverse_transform_output(y_hat_norm.detach().cpu().numpy())\n",
    "        \n",
    "        self.test_preds.append(y_hat_denorm)\n",
    "        self.test_targets.append(y_true_denorm.detach().cpu().numpy()) \n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        if not self.test_preds or not self.test_targets: \n",
    "            return\n",
    "\n",
    "        preds_epoch = np.concatenate(self.test_preds, axis=0)\n",
    "        trues_epoch = np.concatenate(self.test_targets, axis=0)\n",
    "\n",
    "        if self.normalizer is None: self.normalizer = self._get_normalizer_from_datamodule()\n",
    "\n",
    "        self._evaluate(preds_epoch, trues_epoch, phase=\"test\")\n",
    "        self._save_submission(preds_epoch) \n",
    "        \n",
    "        self.test_preds.clear()\n",
    "        self.test_targets.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def _evaluate(self, preds_np, trues_np, phase=\"val\"):\n",
    "        \"\"\"Calculates and logs evaluation metrics.\"\"\"\n",
    "        # This check is important for when _evaluate might be called outside trainer.fit/test context\n",
    "        # or if datamodule is not correctly propagated.\n",
    "        if self.trainer.datamodule is None or not hasattr(self.trainer.datamodule, 'get_lat_weights'):\n",
    "            print(\"Warning: self.trainer.datamodule not fully available in _evaluate. Using fallback for coords/weights.\")\n",
    "            dm_eval = ClimateDataModule(**config[\"data\"]) # Re-init for coords, assumes global config\n",
    "            dm_eval.prepare_data()\n",
    "            dm_eval.setup(stage=phase) # Setup for the correct stage\n",
    "            area_weights = dm_eval.get_lat_weights()\n",
    "            lat, lon = dm_eval.get_coords()\n",
    "            output_vars = dm_eval.hparams.output_vars\n",
    "        else:\n",
    "            area_weights = self.trainer.datamodule.get_lat_weights()\n",
    "            lat, lon = self.trainer.datamodule.get_coords()\n",
    "            output_vars = self.trainer.datamodule.hparams.output_vars\n",
    "\n",
    "\n",
    "        time_coords = np.arange(preds_np.shape[0])\n",
    "        metrics_summary = {}\n",
    "\n",
    "        for i, var_name in enumerate(output_vars):\n",
    "            p_var = preds_np[:, i] \n",
    "            t_var = trues_np[:, i] \n",
    "            \n",
    "            p_xr = xr.DataArray(p_var, dims=[\"time\", \"y\", \"x\"], coords={\"time\": time_coords, \"y\": lat, \"x\": lon})\n",
    "            t_xr = xr.DataArray(t_var, dims=[\"time\", \"y\", \"x\"], coords={\"time\": time_coords, \"y\": lat, \"x\": lon})\n",
    "\n",
    "            rmse = np.sqrt(((p_xr - t_xr) ** 2).weighted(area_weights).mean()).item()\n",
    "            mean_rmse = np.sqrt(((p_xr.mean(\"time\") - t_xr.mean(\"time\")) ** 2).weighted(area_weights).mean()).item()\n",
    "            std_mae = np.abs(p_xr.std(\"time\") - t_xr.std(\"time\")).weighted(area_weights).mean().item()\n",
    "\n",
    "            print(f\"[{phase.upper()}] {var_name}: RMSE={rmse:.4f}, Time-Mean RMSE={mean_rmse:.4f}, Time-Stddev MAE={std_mae:.4f}\")\n",
    "            \n",
    "            metrics_summary[f\"{phase}/{var_name}/rmse\"] = rmse\n",
    "            metrics_summary[f\"{phase}/{var_name}/time_mean_rmse\"] = mean_rmse\n",
    "            metrics_summary[f\"{phase}/{var_name}/time_std_mae\"] = std_mae\n",
    "        \n",
    "        self.log_dict(metrics_summary, logger=True)\n",
    "\n",
    "    def _save_submission(self, predictions_np):\n",
    "        \"\"\"Saves model predictions to a CSV file in Kaggle submission format.\"\"\"\n",
    "        if self.trainer.datamodule is None or not hasattr(self.trainer.datamodule, 'get_coords'):\n",
    "            print(\"Warning: self.trainer.datamodule not fully available in _save_submission. Using fallback.\")\n",
    "            dm_submission = ClimateDataModule(**config[\"data\"])\n",
    "            dm_submission.prepare_data()\n",
    "            dm_submission.setup(stage=\"test\") # Ensure coords are loaded\n",
    "            lat, lon = dm_submission.get_coords()\n",
    "            output_vars = dm_submission.hparams.output_vars\n",
    "        else:\n",
    "            lat, lon = self.trainer.datamodule.get_coords()\n",
    "            output_vars = self.trainer.datamodule.hparams.output_vars\n",
    "            \n",
    "        time_coords_submission = np.arange(predictions_np.shape[0])\n",
    "\n",
    "        rows = []\n",
    "        for t_idx, t_val in enumerate(time_coords_submission):\n",
    "            for var_idx, var_name in enumerate(output_vars):\n",
    "                for y_idx, y_val in enumerate(lat):\n",
    "                    for x_idx, x_val in enumerate(lon):\n",
    "                        row_id = f\"t{t_idx:03d}_{var_name}_{y_val:.2f}_{x_val:.2f}\"\n",
    "                        pred_value = predictions_np[t_idx, var_idx, y_idx, x_idx]\n",
    "                        rows.append({\"ID\": row_id, \"Prediction\": pred_value})\n",
    "\n",
    "        submission_df = pd.DataFrame(rows)\n",
    "        submission_dir = \"submissions\"\n",
    "        os.makedirs(submission_dir, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filepath = os.path.join(submission_dir, f\"kaggle_submission_unet_{timestamp}.csv\")\n",
    "        submission_df.to_csv(filepath, index=False)\n",
    "        print(f\"Submission saved to: {filepath}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0de72c4-c536-470a-8a26-e4a73a666f85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruz039/.local/lib/python3.11/site-packages/lightning/fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting U-Net model training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 04:55:57.909004: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-17 04:55:57.949198: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-17 04:55:57.949245: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-17 04:55:57.950294: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-17 04:55:57.957371: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset with 2943 samples...\n",
      "Dataset created. Input shape: torch.Size([2943, 5, 48, 72]), Output shape: torch.Size([2943, 2, 48, 72])\n",
      "Creating dataset with 120 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created. Input shape: torch.Size([120, 5, 48, 72]), Output shape: torch.Size([120, 2, 48, 72])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type    | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | model     | ResNet  | 11.2 M | train\n",
      "1 | criterion | MSELoss | 0      | train\n",
      "----------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.684    Total estimated model params size (MB)\n",
      "69        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/ruz039/.local/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (46) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "040ebd8a4375444d958c4c743e6ec348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved. New best score: 82.380\n",
      "Epoch 0, global step 46: 'val/loss' reached 82.37978 (best 82.37978), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/version_1/checkpoints/unet-best-epoch=00-val/loss=82.38.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=175.5129, Time-Mean RMSE=96.2518, Time-Stddev MAE=128.9028\n",
      "[VAL] pr: RMSE=31.8142, Time-Mean RMSE=26.1904, Time-Stddev MAE=14.3366\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 61.658 >= min_delta = 0.0. New best score: 20.722\n",
      "Epoch 1, global step 92: 'val/loss' reached 20.72213 (best 20.72213), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/version_1/checkpoints/unet-best-epoch=01-val/loss=20.72.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=96.0668, Time-Mean RMSE=58.5966, Time-Stddev MAE=67.0241\n",
      "[VAL] pr: RMSE=14.8811, Time-Mean RMSE=9.2109, Time-Stddev MAE=8.3847\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 6.616 >= min_delta = 0.0. New best score: 14.106\n",
      "Epoch 2, global step 138: 'val/loss' reached 14.10643 (best 14.10643), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/version_1/checkpoints/unet-best-epoch=02-val/loss=14.11.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=64.6294, Time-Mean RMSE=25.7538, Time-Stddev MAE=53.9040\n",
      "[VAL] pr: RMSE=14.2202, Time-Mean RMSE=9.9704, Time-Stddev MAE=6.9772\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.245 >= min_delta = 0.0. New best score: 13.861\n",
      "Epoch 3, global step 184: 'val/loss' reached 13.86138 (best 13.86138), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/version_1/checkpoints/unet-best-epoch=03-val/loss=13.86.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=69.7610, Time-Mean RMSE=50.4383, Time-Stddev MAE=43.5849\n",
      "[VAL] pr: RMSE=13.8794, Time-Mean RMSE=9.3984, Time-Stddev MAE=6.9822\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 6.226 >= min_delta = 0.0. New best score: 7.636\n",
      "Epoch 4, global step 230: 'val/loss' reached 7.63560 (best 7.63560), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/version_1/checkpoints/unet-best-epoch=04-val/loss=7.64.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=51.4174, Time-Mean RMSE=28.6953, Time-Stddev MAE=38.1924\n",
      "[VAL] pr: RMSE=9.8416, Time-Mean RMSE=4.7257, Time-Stddev MAE=5.5358\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 1.621 >= min_delta = 0.0. New best score: 6.014\n",
      "Epoch 5, global step 276: 'val/loss' reached 6.01449 (best 6.01449), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/version_1/checkpoints/unet-best-epoch=05-val/loss=6.01.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=47.3929, Time-Mean RMSE=24.5678, Time-Stddev MAE=36.3817\n",
      "[VAL] pr: RMSE=8.4873, Time-Mean RMSE=3.4949, Time-Stddev MAE=4.6664\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 1.082 >= min_delta = 0.0. New best score: 4.933\n",
      "Epoch 6, global step 322: 'val/loss' reached 4.93287 (best 4.93287), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/version_1/checkpoints/unet-best-epoch=06-val/loss=4.93.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=43.0287, Time-Mean RMSE=16.1971, Time-Stddev MAE=35.4112\n",
      "[VAL] pr: RMSE=7.2959, Time-Mean RMSE=2.8425, Time-Stddev MAE=3.7610\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 1.030 >= min_delta = 0.0. New best score: 3.903\n",
      "Epoch 7, global step 368: 'val/loss' reached 3.90281 (best 3.90281), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/version_1/checkpoints/unet-best-epoch=07-val/loss=3.90.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=39.0149, Time-Mean RMSE=15.5968, Time-Stddev MAE=31.4980\n",
      "[VAL] pr: RMSE=6.6639, Time-Mean RMSE=2.4375, Time-Stddev MAE=3.2626\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.292 >= min_delta = 0.0. New best score: 3.611\n",
      "Epoch 8, global step 414: 'val/loss' reached 3.61073 (best 3.61073), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/version_1/checkpoints/unet-best-epoch=08-val/loss=3.61.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=38.4802, Time-Mean RMSE=22.0491, Time-Stddev MAE=27.3383\n",
      "[VAL] pr: RMSE=6.3529, Time-Mean RMSE=2.3403, Time-Stddev MAE=3.0168\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.338 >= min_delta = 0.0. New best score: 3.272\n",
      "Epoch 9, global step 460: 'val/loss' reached 3.27225 (best 3.27225), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/version_1/checkpoints/unet-best-epoch=09-val/loss=3.27.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=33.1929, Time-Mean RMSE=13.4367, Time-Stddev MAE=26.3828\n",
      "[VAL] pr: RMSE=6.3840, Time-Mean RMSE=2.9815, Time-Stddev MAE=2.8440\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.526 >= min_delta = 0.0. New best score: 2.746\n",
      "Epoch 10, global step 506: 'val/loss' reached 2.74612 (best 2.74612), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/version_1/checkpoints/unet-best-epoch=10-val/loss=2.75.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=30.3918, Time-Mean RMSE=15.8568, Time-Stddev MAE=22.1174\n",
      "[VAL] pr: RMSE=5.8608, Time-Mean RMSE=3.0451, Time-Stddev MAE=2.4114\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 552: 'val/loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=32.3497, Time-Mean RMSE=14.6957, Time-Stddev MAE=24.5223\n",
      "[VAL] pr: RMSE=5.6914, Time-Mean RMSE=2.1534, Time-Stddev MAE=2.5816\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 598: 'val/loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=32.1158, Time-Mean RMSE=16.8699, Time-Stddev MAE=23.1382\n",
      "[VAL] pr: RMSE=6.1809, Time-Mean RMSE=3.0482, Time-Stddev MAE=2.5092\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.203 >= min_delta = 0.0. New best score: 2.543\n",
      "Epoch 13, global step 644: 'val/loss' reached 2.54271 (best 2.54271), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/version_1/checkpoints/unet-best-epoch=13-val/loss=2.54.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=32.1272, Time-Mean RMSE=15.3960, Time-Stddev MAE=23.8279\n",
      "[VAL] pr: RMSE=5.3746, Time-Mean RMSE=2.5733, Time-Stddev MAE=2.0959\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25dddef9b73a49ed86425be1920cdd34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.168 >= min_delta = 0.0. New best score: 2.375\n",
      "Epoch 14, global step 690: 'val/loss' reached 2.37461 (best 2.37461), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/version_1/checkpoints/unet-best-epoch=14-val/loss=2.37.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=27.1474, Time-Mean RMSE=10.8584, Time-Stddev MAE=21.2413\n",
      "[VAL] pr: RMSE=5.6638, Time-Mean RMSE=3.3242, Time-Stddev MAE=1.9112\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5119db52405d4a2d99213c38613e1c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 736: 'val/loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=33.0648, Time-Mean RMSE=18.6756, Time-Stddev MAE=23.2095\n",
      "[VAL] pr: RMSE=5.0511, Time-Mean RMSE=2.4356, Time-Stddev MAE=1.9398\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "650b4d5af12b4a879aeacaa6c93c48f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.542 >= min_delta = 0.0. New best score: 1.833\n",
      "Epoch 16, global step 782: 'val/loss' reached 1.83282 (best 1.83282), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/version_1/checkpoints/unet-best-epoch=16-val/loss=1.83.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=26.2800, Time-Mean RMSE=12.7070, Time-Stddev MAE=18.9646\n",
      "[VAL] pr: RMSE=4.7693, Time-Mean RMSE=2.0581, Time-Stddev MAE=1.9058\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.027 >= min_delta = 0.0. New best score: 1.806\n",
      "Epoch 17, global step 828: 'val/loss' reached 1.80576 (best 1.80576), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/version_1/checkpoints/unet-best-epoch=17-val/loss=1.81.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=24.1528, Time-Mean RMSE=10.6463, Time-Stddev MAE=17.7292\n",
      "[VAL] pr: RMSE=4.7550, Time-Mean RMSE=1.9804, Time-Stddev MAE=1.8772\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.083 >= min_delta = 0.0. New best score: 1.723\n",
      "Epoch 18, global step 874: 'val/loss' reached 1.72292 (best 1.72292), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/version_1/checkpoints/unet-best-epoch=18-val/loss=1.72.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=25.0657, Time-Mean RMSE=12.7579, Time-Stddev MAE=17.1890\n",
      "[VAL] pr: RMSE=4.5996, Time-Mean RMSE=1.9392, Time-Stddev MAE=1.8118\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 920: 'val/loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=24.0107, Time-Mean RMSE=13.0061, Time-Stddev MAE=16.2700\n",
      "[VAL] pr: RMSE=4.8181, Time-Mean RMSE=2.6075, Time-Stddev MAE=1.7232\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.209 >= min_delta = 0.0. New best score: 1.514\n",
      "Epoch 20, global step 966: 'val/loss' reached 1.51357 (best 1.51357), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/version_1/checkpoints/unet-best-epoch=20-val/loss=1.51.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=22.9894, Time-Mean RMSE=12.4624, Time-Stddev MAE=15.6717\n",
      "[VAL] pr: RMSE=4.3443, Time-Mean RMSE=2.0835, Time-Stddev MAE=1.6128\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, global step 1012: 'val/loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=33.0565, Time-Mean RMSE=20.0313, Time-Stddev MAE=19.5996\n",
      "[VAL] pr: RMSE=4.6948, Time-Mean RMSE=2.4165, Time-Stddev MAE=1.7833\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.234 >= min_delta = 0.0. New best score: 1.279\n",
      "Epoch 22, global step 1058: 'val/loss' reached 1.27941 (best 1.27941), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/version_1/checkpoints/unet-best-epoch=22-val/loss=1.28.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=20.9029, Time-Mean RMSE=11.0936, Time-Stddev MAE=13.5406\n",
      "[VAL] pr: RMSE=4.1901, Time-Mean RMSE=1.8432, Time-Stddev MAE=1.5721\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23, global step 1104: 'val/loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=22.4695, Time-Mean RMSE=12.0208, Time-Stddev MAE=14.7622\n",
      "[VAL] pr: RMSE=4.4507, Time-Mean RMSE=2.2024, Time-Stddev MAE=1.6501\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24, global step 1150: 'val/loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=21.0052, Time-Mean RMSE=10.3462, Time-Stddev MAE=14.3447\n",
      "[VAL] pr: RMSE=4.3527, Time-Mean RMSE=2.1096, Time-Stddev MAE=1.4988\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25, global step 1196: 'val/loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=19.3907, Time-Mean RMSE=9.0492, Time-Stddev MAE=12.8699\n",
      "[VAL] pr: RMSE=4.4462, Time-Mean RMSE=2.2979, Time-Stddev MAE=1.6927\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26, global step 1242: 'val/loss' was not in top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=20.4186, Time-Mean RMSE=10.7820, Time-Stddev MAE=12.9301\n",
      "[VAL] pr: RMSE=4.4944, Time-Mean RMSE=2.5007, Time-Stddev MAE=1.5346\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.099 >= min_delta = 0.0. New best score: 1.180\n",
      "Epoch 27, global step 1288: 'val/loss' reached 1.17992 (best 1.17992), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/version_1/checkpoints/unet-best-epoch=27-val/loss=1.18.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=18.6279, Time-Mean RMSE=9.1363, Time-Stddev MAE=12.7494\n",
      "[VAL] pr: RMSE=4.1363, Time-Mean RMSE=2.0824, Time-Stddev MAE=1.4988\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.011 >= min_delta = 0.0. New best score: 1.168\n",
      "Epoch 28, global step 1334: 'val/loss' reached 1.16847 (best 1.16847), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/version_1/checkpoints/unet-best-epoch=28-val/loss=1.17.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=17.6856, Time-Mean RMSE=7.9444, Time-Stddev MAE=11.9060\n",
      "[VAL] pr: RMSE=4.2563, Time-Mean RMSE=2.0761, Time-Stddev MAE=1.4392\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val/loss improved by 0.013 >= min_delta = 0.0. New best score: 1.155\n",
      "Epoch 29, global step 1380: 'val/loss' reached 1.15498 (best 1.15498), saving model to '/home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/version_1/checkpoints/unet-best-epoch=29-val/loss=1.15.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=18.7086, Time-Mean RMSE=10.0719, Time-Stddev MAE=11.6510\n",
      "[VAL] pr: RMSE=4.1701, Time-Mean RMSE=2.0551, Time-Stddev MAE=1.4993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "Starting U-Net model testing using the best checkpoint...\n",
      "Creating dataset with 120 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/version_1/checkpoints/unet-best-epoch=29-val/loss=1.15.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created. Input shape: torch.Size([120, 5, 48, 72]), Output shape: torch.Size([120, 2, 48, 72])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at /home/ruz039/private/cse151b/DL_for_Climate_Emulation/lightning_logs/version_1/checkpoints/unet-best-epoch=29-val/loss=1.15.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19798df499dc41fba6527db0155151fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] tas: RMSE=285.8640, Time-Mean RMSE=284.3571, Time-Stddev MAE=27.7675\n",
      "[TEST] pr: RMSE=7.8771, Time-Mean RMSE=4.3515, Time-Stddev MAE=5.8766\n",
      "Submission saved to: submissions/kaggle_submission_unet_20250517_045746.csv\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "       Test metric             DataLoader 0\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "      test/pr/rmse           7.877082347869873\n",
      " test/pr/time_mean_rmse      4.351463317871094\n",
      "  test/pr/time_std_mae       5.876616477966309\n",
      "      test/tas/rmse          285.8639831542969\n",
      " test/tas/time_mean_rmse     284.3570861816406\n",
      "  test/tas/time_std_mae     27.767528533935547\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "Testing finished.\n",
      "Test Results: [{'test/tas/rmse': 285.8639831542969, 'test/tas/time_mean_rmse': 284.3570861816406, 'test/tas/time_std_mae': 27.767528533935547, 'test/pr/rmse': 7.877082347869873, 'test/pr/time_mean_rmse': 4.351463317871094, 'test/pr/time_std_mae': 5.876616477966309}]\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Training and Evaluation Script\n",
    "\n",
    "# --- Instantiate DataModule ---\n",
    "datamodule = ClimateDataModule(**config[\"data\"])\n",
    "# datamodule.prepare_data() # Called by Trainer when .fit() or .test() is called\n",
    "# datamodule.setup()      # Called by Trainer when .fit() or .test() is called\n",
    "\n",
    "# --- Instantiate U-Net Model ---\n",
    "n_inputs = len(config[\"data\"][\"input_vars\"])\n",
    "n_outputs = len(config[\"data\"][\"output_vars\"])\n",
    "\n",
    "unet_config_params = config.get(\"model_unet\", {}) \n",
    "init_features = unet_config_params.get(\"init_features\", 64)\n",
    "bilinear_upsampling = unet_config_params.get(\"bilinear\", True)\n",
    "\n",
    "resnet_model = ResNet(depth        = 18,\n",
    "                      n_input_channels = n_inputs,\n",
    "                      n_output_classes = n_outputs)\n",
    "\n",
    "# --- Instantiate Lightning Module ---\n",
    "learning_rate = config[\"training\"][\"lr\"]\n",
    "lightning_module = ClimateEmulationModule(resnet_model, learning_rate=learning_rate)\n",
    "\n",
    "# --- Setup Trainer ---\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val/loss\", \n",
    "    mode=\"min\",         \n",
    "    filename=\"unet-best-{epoch:02d}-{val/loss:.2f}\", \n",
    "    save_top_k=1,       \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val/loss\",\n",
    "    patience=5, \n",
    "    verbose=True,\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "trainer_params = {**config[\"trainer\"]} \n",
    "trainer_params[\"callbacks\"] = [checkpoint_callback, early_stop_callback]\n",
    "# Optional: Add logger\n",
    "# from lightning.pytorch.loggers import TensorBoardLogger\n",
    "# logger = TensorBoardLogger(\"tb_logs\", name=\"unet_climate_emulation\")\n",
    "# trainer_params[\"logger\"] = logger\n",
    "\n",
    "trainer = pl.Trainer(**trainer_params)\n",
    "\n",
    "# --- Train the Model ---\n",
    "print(\"Starting U-Net model training...\")\n",
    "trainer.fit(lightning_module, datamodule=datamodule)\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# --- Test the Model ---\n",
    "print(\"Starting U-Net model testing using the best checkpoint...\")\n",
    "# trainer.test will use the checkpoint_callback's best_model_path by default if available\n",
    "# or you can specify ckpt_path=\"best\"\n",
    "test_results = trainer.test(lightning_module, datamodule=datamodule, ckpt_path=\"best\") \n",
    "print(\"Testing finished.\")\n",
    "print(\"Test Results:\", test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39ea3c62-3eb2-4a17-bacb-d49850478666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Plotting Utilities (Optional)\n",
    "# Ensure matplotlib, numpy, and xarray are imported (usually in Cell 1)\n",
    "\n",
    "def plot_comparison(true_xr, pred_xr, title, cmap='viridis', diff_cmap='RdBu_r', metric_val=None, metric_name=\"Metric\"):\n",
    "    \"\"\"\n",
    "    Plots a comparison between ground truth, prediction, and their difference.\n",
    "    Includes calculation and display of a spatial mean metric (e.g., RMSE).\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 5)) \n",
    "    fig.suptitle(title, fontsize=16) \n",
    "\n",
    "    common_min = min(true_xr.min().item(), pred_xr.min().item())\n",
    "    common_max = max(true_xr.max().item(), pred_xr.max().item())\n",
    "\n",
    "    true_xr.plot(ax=axs[0], cmap=cmap, vmin=common_min, vmax=common_max, add_colorbar=True, cbar_kwargs={'label': true_xr.name or 'Value'})\n",
    "    axs[0].set_title(f\"Ground Truth\")\n",
    "\n",
    "    pred_xr.plot(ax=axs[1], cmap=cmap, vmin=common_min, vmax=common_max, add_colorbar=True, cbar_kwargs={'label': pred_xr.name or 'Value'})\n",
    "    axs[1].set_title(f\"Prediction\")\n",
    "\n",
    "    diff = pred_xr - true_xr\n",
    "    abs_max_diff = np.max(np.abs(diff.data)) if diff.size > 0 else 0.1 \n",
    "    \n",
    "    diff_plot_params = {'cmap': diff_cmap, 'add_colorbar': True, 'cbar_kwargs': {'label': 'Difference'}}\n",
    "    if abs_max_diff > 0: \n",
    "        diff_plot_params['vmin'] = -abs_max_diff\n",
    "        diff_plot_params['vmax'] = abs_max_diff\n",
    "        \n",
    "    diff.plot(ax=axs[2], **diff_plot_params)\n",
    "    \n",
    "    title_suffix = \"\"\n",
    "    if metric_val is not None:\n",
    "        title_suffix = f\" ({metric_name}: {metric_val:.4f})\"\n",
    "    axs[2].set_title(f\"Difference (Pred - Truth){title_suffix}\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96]) \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08db76cd-147a-45aa-9b40-bfe686da30e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Visualization Script (Optional)\n",
    "\n",
    "try:\n",
    "    val_preds_loaded = np.load(\"val_preds.npy\")\n",
    "    val_trues_loaded = np.load(\"val_trues.npy\")\n",
    "\n",
    "    if not hasattr(datamodule, 'lat') or datamodule.lat is None:\n",
    "        print(\"Datamodule not fully set up for visualization. Setting it up...\")\n",
    "        # datamodule.prepare_data() # Should have been called by trainer\n",
    "        datamodule.setup(stage=\"fit\") # Ensure lat, lon, etc. are available\n",
    "\n",
    "    lat, lon = datamodule.get_coords()\n",
    "    output_vars = config[\"data\"][\"output_vars\"] \n",
    "    area_weights_vis = datamodule.get_lat_weights() \n",
    "    \n",
    "    time_val_coords = np.arange(val_preds_loaded.shape[0])\n",
    "\n",
    "    print(f\"\\n--- Visualizing Validation Predictions for U-Net ---\")\n",
    "    for i, var_name in enumerate(output_vars):\n",
    "        pred_xr = xr.DataArray(val_preds_loaded[:, i], dims=[\"time\", \"y\", \"x\"], \n",
    "                               coords={\"time\": time_val_coords, \"y\": lat, \"x\": lon}, name=var_name)\n",
    "        true_xr = xr.DataArray(val_trues_loaded[:, i], dims=[\"time\", \"y\", \"x\"], \n",
    "                               coords={\"time\": time_val_coords, \"y\": lat, \"x\": lon}, name=var_name)\n",
    "\n",
    "        pred_mean = pred_xr.mean(\"time\")\n",
    "        true_mean = true_xr.mean(\"time\")\n",
    "        mean_rmse_var = np.sqrt(((pred_mean - true_mean) ** 2).weighted(area_weights_vis).mean()).item()\n",
    "        plot_comparison(true_mean, pred_mean, \n",
    "                        f\"U-Net: {var_name.upper()} - Validation Time-Mean\",\n",
    "                        metric_val=mean_rmse_var, metric_name=\"Time-Mean RMSE\")\n",
    "\n",
    "        pred_std = pred_xr.std(\"time\")\n",
    "        true_std = true_xr.std(\"time\")\n",
    "        std_mae_var = np.abs(pred_std - true_std).weighted(area_weights_vis).mean().item()\n",
    "        plot_comparison(true_std, pred_std, \n",
    "                        f\"U-Net: {var_name.upper()} - Validation Time-StdDev\", cmap=\"plasma\",\n",
    "                        metric_val=std_mae_var, metric_name=\"Time-StdDev MAE\")\n",
    "\n",
    "        if len(time_val_coords) > 0:\n",
    "            t_idx_random = np.random.randint(0, len(time_val_coords))\n",
    "            pred_sample = pred_xr.isel(time=t_idx_random)\n",
    "            true_sample = true_xr.isel(time=t_idx_random)\n",
    "            sample_rmse_var = np.sqrt(((pred_sample - true_sample) ** 2).weighted(area_weights_vis).mean()).item()\n",
    "            plot_comparison(true_sample, pred_sample, \n",
    "                            f\"U-Net: {var_name.upper()} - Validation Sample (Timestep {t_idx_random})\",\n",
    "                            metric_val=sample_rmse_var, metric_name=\"RMSE\")\n",
    "        else:\n",
    "            print(f\"No time steps available in validation predictions for {var_name} to plot a random sample.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"val_preds.npy or val_trues.npy not found. \"\n",
    "          \"Ensure that the training and validation loop (trainer.fit) has been run successfully, \"\n",
    "          \"and the on_validation_epoch_end method in ClimateEmulationModule saved these files.\")\n",
    "except AttributeError as e:\n",
    "    print(f\"AttributeError during visualization: {e}. Ensure datamodule is correctly initialized and set up.\")\n",
    "    print(\"This might happen if 'datamodule' from the training cell is not in scope or wasn't fully set up.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during visualization: {e}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
